{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2ecfca",
   "metadata": {},
   "source": [
    "# Chapter 11 Python Projects\n",
    "\n",
    "## Using argparse\n",
    "\n",
    "\n",
    "argparse is a Python module that makes it easy to write command-line interfaces. It allows you to specify the arguments that your program should accept in a clean and organized way, and it takes care of handling the input and output for you.\n",
    "\n",
    "Here's an example of how to use argparse to parse command-line arguments in a Python script:\n",
    "\n",
    "\n",
    "```python\n",
    "argparse is a Python module that makes it easy to write command-line interfaces. It allows you to specify the arguments that your program should accept in a clean and organized way, and it takes care of handling the input and output for you.\n",
    "\n",
    "Here's an example of how to use argparse to parse command-line arguments in a Python script:\n",
    "```\n",
    "\n",
    "This script defines a single command-line argument, --sum, which specifies whether the program should sum or find the maximum of the integers. If the --sum argument is not provided, the default action is to find the maximum.\n",
    "\n",
    "To run this script, you would type something like python script.py 1 2 3 --sum at the command line. This would cause the script to print 6, which is the sum of the integers 1, 2, and 3.\n",
    "\n",
    "Here's an example of how you might use argparse to write a command-line interface for a program that keeps track of the books that a bear has read:\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Bear book tracker')\n",
    "\n",
    "# Add a command to add a book to the bear's reading list\n",
    "parser.add_argument('--add', dest='book', type=str,\n",
    "                    help='add a book to the reading list')\n",
    "\n",
    "# Add a command to list the books that the bear has read\n",
    "parser.add_argument('--list', action='store_true',\n",
    "                    help='list the books that the bear has read')\n",
    "\n",
    "# Parse the arguments\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Initialize an empty list to store the bear's books\n",
    "books = []\n",
    "\n",
    "# If the --add argument was provided, add the book to the list\n",
    "if args.book:\n",
    "    books.append(args.book)\n",
    "\n",
    "# If the --list argument was provided, print the list of books\n",
    "if args.list:\n",
    "    print(\"Bear's books:\")\n",
    "    for book in books:\n",
    "        print(book)\n",
    "\n",
    "```\n",
    "\n",
    "To use this program, you would type something like python book_tracker.py --add \"The Giving Tree\" to add a book to the bear's reading list, or python book_tracker.py --list to view the list of books.\n",
    "\n",
    "I hope this helps give you a basic understanding of how argparse works!\n",
    "\n",
    "## Convert pdf to pngs\n",
    "ImageMagick is a software suite to create, edit, and compose bitmap images. It can read, convert and write images in a variety of formats (over 100) including DPX, EXR, GIF, JPEG, JPEG-2000, PDF, PhotoCD, PNG, Postscript, SVG, and TIFF. ImageMagick is used to translate, flip, mirror, rotate, scale, shear and transform images, adjust image colors, apply various special effects, or draw text, lines, polyglines, ellipses and BÃ©zier curves.\n",
    "\n",
    "It is used by graphic designers, photographers, scientists and also used for creating thumbnails for websites, creating GIF animations, converting PDF pages to images and so on. ImageMagick can be used from the command-line or can be used as a programming library for software development.\n",
    "\n",
    "\n",
    "You can call ImageMagick from Python using the `os` module, which provides a way to interface with the underlying operating system.\n",
    "\n",
    "Here's an example of how to use ImageMagick to convert an image from one format to another:\n",
    "\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "input_image = 'input.jpg'\n",
    "output_image = 'output.png'\n",
    "\n",
    "os.system(f'convert {input_image} {output_image}')\n",
    "```\n",
    "In this example, `convert` is the command-line utility provided by ImageMagick to convert images. The `input_image` variable specifies the path to the input image and `output_image` specifies the path to the output image. The `os.system` function is used to run the `convert` command, which converts the input image to the output image.\n",
    "\n",
    "The convert utility is only available in imagemagick 6.\n",
    "\n",
    "Note that this method is prone to security vulnerabilities, since it passes the parameters directly to the shell. To avoid these vulnerabilities, you can use the `subprocess` module, which provides a more secure way of calling shell commands. Here's an example of how to use the `subprocess` module to call ImageMagick:\n",
    "\n",
    "\n",
    "```python\n",
    "import subprocess\n",
    "\n",
    "input_image = 'input.jpg'\n",
    "output_image = 'output.png'\n",
    "\n",
    "subprocess.run(['convert', input_image, output_image])\n",
    "```\n",
    "In this example, the `subprocess.run` function is used to run the `convert` command, and the parameters are passed as a list, rather than as a string. This is a more secure way of calling shell commands, as it avoids shell injection attacks.\n",
    "\n",
    "\n",
    "You can call ImageMagick from Python using the `os` module, which provides a way to interface with the underlying operating system.\n",
    "\n",
    "Here's an example of how to use ImageMagick to convert an image from one format to another:\n",
    "\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "input_image = 'input.jpg'\n",
    "output_image = 'output.png'\n",
    "\n",
    "os.system(f'convert {input_image} {output\\_image}')`\n",
    "```\n",
    "In this example, `convert` is the command-line utility provided by ImageMagick to convert images. The `input_image` variable specifies the path to the input image and `output_image` specifies the path to the output image. The `os.system` function is used to run the `convert` command, which converts the input image to the output image.\n",
    "\n",
    "Note that this method is prone to security vulnerabilities, since it passes the parameters directly to the shell. To avoid these vulnerabilities, you can use the `subprocess` module, which provides a more secure way of calling shell commands. Here's an example of how to use the `subprocess` module to call ImageMagick:\n",
    "\n",
    "\n",
    "```python\n",
    "import subprocess\n",
    "\n",
    "input_image = 'input.jpg'\n",
    "output_image = 'output.png'\n",
    "\n",
    "subprocess.run(['convert', input_image, output_image])`\n",
    "```\n",
    "In this example, the `subprocess.run` function is used to run the `convert` command, and the parameters are passed as a list, rather than as a string. This is a more secure way of calling shell commands, as it avoids shell injection attacks.\n",
    "\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    dir_list =  os.listdir('.')\n",
    "    for full_file_name in dir_list:\n",
    "        base_name, extension = os.path.splitext(full_file_name)\n",
    "        if extension == '.pdf': # then .pdf file --> convert to image!\n",
    "            cmd_str = ' '.join(['convert',\n",
    "                                '-density 400',\n",
    "                                full_file_name,\n",
    "                                '-flatten',\n",
    "                                base_name + '.png'])\n",
    "            print(cmd_str)  # echo command to terminal\n",
    "            os.system(cmd_str)  # execute command\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "This is a Python script that uses the os module to convert all PDF files in the current directory to PNG images using ImageMagick. The main function lists all the files in the current directory using os.listdir, and then loops through each file name in dir_list. If the file extension is '.pdf', the script splits the file name into its base name and extension using os.path.splitext, and then creates a command string using cmd_str that calls ImageMagick's convert utility with the specified options.\n",
    "\n",
    "The options used in this script are:\n",
    "\n",
    "- density 400: sets the density of the output image, which affects its resolution.\n",
    "- flatten: combines all the layers of the PDF into a single image.\n",
    "\n",
    "The os.system function is then used to execute the command string, which converts the PDF file to a PNG image. The script prints the command string to the terminal to show what it's doing, and the result is a set of PNG images, one for each PDF file in the directory.\n",
    "\n",
    "ImageMagick is a versatile and powerful software suite that has many uses, including:\n",
    "\n",
    "1. Image Conversion: ImageMagick can be used to convert images from one format to another, making it easier to work with different image formats.\n",
    "2. Image Editing: ImageMagick can be used to perform basic image editing operations, such as cropping, resizing, rotating, and flipping images.\n",
    "3. Image Compression: ImageMagick can be used to compress images, reducing their file size while maintaining image quality.\n",
    "4. Image Manipulation: ImageMagick can be used to manipulate images in various ways, such as applying special effects, adding text and shapes, and blending multiple images together.\n",
    "5. Thumbnail Creation: ImageMagick can be used to create thumbnails for websites, making it easier to manage large numbers of images.\n",
    "6. Image Animation: ImageMagick can be used to create GIF animations from a sequence of images.\n",
    "7. PDF Conversion: ImageMagick can be used to convert PDF pages to images, making it easier to work with PDF documents.\n",
    "8. Image Processing: ImageMagick can be used for image processing tasks, such as adjusting image colors and removing noise from images.\n",
    "9. Scientific Visualization: ImageMagick can be used in scientific visualization, such as for rendering 3D models or visualizing data.\n",
    "\n",
    "These are just a few examples of the many uses of ImageMagick. It is a highly versatile software suite that can be used in many different ways to work with images.\n",
    "\n",
    "\n",
    "### References\n",
    "- https://github.com/FriendlyUser/ELEC370/blob/master/Assignments/A1/html/pdfToPNGMagick.py\n",
    "\n",
    "\n",
    "## Chat gpt blog generation\n",
    "ChatGPT is a large language model that has been trained using a technique called unsupervised learning, in which the model learns to generate text by being fed a large dataset of text. It is designed to generate human-like responses to prompts, and can be used for a variety of natural language processing tasks, such as language translation, text summarization, and question answering.\n",
    "\n",
    "\n",
    "```python \n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from pyChatGPT import ChatGPT\n",
    "# import yaml\n",
    "from yaml import load, dump, Loader\n",
    "import io\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from stability_sdk import client\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "# Our Host URL should not be prepended with \"https\" nor should it have a trailing slash.\n",
    "os.environ['STABILITY_HOST'] = 'grpc.stability.ai:443'\n",
    "os.environ[\"STABILITY_KEY\"] = os.getenv(\"STABILITY_KEY\")\n",
    "\n",
    "def generate_image(cfg:dict)-> None:\n",
    "    prompt = cfg['imageArgs']['prompt']\n",
    "    stability_api = client.StabilityInference(\n",
    "        key=os.environ['STABILITY_KEY'], # API Key reference.\n",
    "        verbose=True, # Print debug messages.\n",
    "        engine=\"stable-diffusion-v1-5\", # Set the engine to use for generation. \n",
    "        # Available engines: stable-diffusion-v1 stable-diffusion-v1-5 stable-diffusion-512-v2-0 stable-diffusion-768-v2-0 \n",
    "        # stable-diffusion-512-v2-1 stable-diffusion-768-v2-1 stable-inpainting-v1-0 stable-inpainting-512-v2-0\n",
    "    )\n",
    "    answers = stability_api.generate(\n",
    "        prompt=prompt\n",
    "    )\n",
    "    for resp in answers:\n",
    "        for artifact in resp.artifacts:\n",
    "            if artifact.finish_reason == generation.FILTER:\n",
    "                warnings.warn(\n",
    "                    \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                    \"Please modify the prompt and try again.\")\n",
    "            if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "                img = Image.open(io.BytesIO(artifact.binary))\n",
    "                # images\n",
    "                img.save(\"images\" + \"/\" + str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename.\n",
    "\n",
    "def generate_frontmatter(cfg: dict)-> None:\n",
    "    # open output file\n",
    "    output_file = cfg['outputFile']\n",
    "    with open(output_file, 'w') as f:\n",
    "        # write frontmatter\n",
    "        f.write('---\\n')\n",
    "        # get frontmatter\n",
    "        frontmatter = cfg['frontMatter']\n",
    "        for key, value in frontmatter.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "        f.write('---\\n')\n",
    "        # write body\n",
    "    pass\n",
    "\n",
    "def use_programming_language(cfg: dict, section_text: str)-> None:\n",
    "    # get programming language\n",
    "    programming_language = cfg['programmingLanguage']\n",
    "    # use regex to find plain ``` and replace with ```programming_language\n",
    "    type_start_line = None\n",
    "    type_end_line = None\n",
    "    # replace with code blocks with programming language\n",
    "    # split lines by \\n\n",
    "    modified_lines = []\n",
    "    lines = section_text.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == '```':\n",
    "            if type_start_line is None:\n",
    "                type_start_line = i\n",
    "                # append modified line\n",
    "                modified_lines.append(f'```{programming_language}')\n",
    "            else:\n",
    "                # odd number of ``` so we are ending a code block\n",
    "                type_start_line = None\n",
    "                # append modified line\n",
    "                modified_lines.append('```')\n",
    "        else:\n",
    "            # adjust line if type_start_line is not None\n",
    "            # remove ` at the beginning of the line and the end of the line\n",
    "            if type_start_line is not None:\n",
    "                # check for ` character at the beginning of the line\n",
    "                if line[0] == '`':\n",
    "                    line = line[1:]\n",
    "                # check for ` character at the end of the line\n",
    "                if line[-1] == '`':\n",
    "                    line = line[:-1]\n",
    "            modified_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(modified_lines)\n",
    "\n",
    "def generate_body(cfg: dict)-> None:\n",
    "    # read CHATGPT_TOKEN from os\n",
    "    CHATGPT_SESSION_TOKEN = os.getenv(\"CHATGPT_TOKEN\")\n",
    "    # print(CHATGPT_SESSION_TOKEN)\n",
    "    session_token = CHATGPT_SESSION_TOKEN  # `__Secure-next-auth.session-token` cookie from https://chat.openai.com/chat\n",
    "    api = ChatGPT(session_token)  # auth with session token\n",
    "    output_file = cfg['outputFile']\n",
    "    sections = cfg['sections']\n",
    "    with open(output_file, 'a') as f:\n",
    "        for section in sections:\n",
    "            # check if str or dict\n",
    "            if isinstance(section, str):\n",
    "                resp = api.send_message(section)\n",
    "                clean_message = use_programming_language(cfg, resp['message'])\n",
    "            else:\n",
    "                # assume dict\n",
    "                # type and src from dict\n",
    "                input_type = section['type']\n",
    "                src = section['src']\n",
    "                if input_type == 'file':\n",
    "                    # read src from file\n",
    "                    with open(src, 'r') as src_file:\n",
    "                        src_text = src_file.read()\n",
    "                    resp = api.send_message(src_text)\n",
    "                    clean_message = resp['message']\n",
    "                    print(clean_message)\n",
    "                    # write original text\n",
    "                    programming_language = cfg['programmingLanguage']\n",
    "                    f.write(f\"```{programming_language} \\n {src_text} \\n ```\\n\")\n",
    "                    f.write('\\n')\n",
    "            f.write(clean_message)\n",
    "            f.write('\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--file', type=str, default='posts/chatgpt_blog_generation.yml')\n",
    "    args = parser.parse_args()\n",
    "    # valid files exist\n",
    "    # argparse for file eventually\n",
    "    with open(args.file, 'r') as f:\n",
    "        cfg = load(f, Loader=Loader)\n",
    "    # if file exists skip\n",
    "    # if not run these functions\n",
    "    if os.path.exists(cfg['outputFile']):\n",
    "        print('file exists')\n",
    "        exit(0)\n",
    "    # if genImage is true, then makeImage\n",
    "    if cfg['imageArgs']:\n",
    "        generate_image(cfg)\n",
    "        pass\n",
    "\n",
    "    generate_frontmatter(cfg)\n",
    "    generate_body(cfg) \n",
    " ```\n",
    "\n",
    "This is a Python script that uses the ChatGPT API, the pyChatGPT library, and the stability\\_sdk to generate text and images. The script takes in a configuration file in YAML format, which specifies the input text, the output file, and various settings such as the programming language to use for code blocks and the prompt for the image generation.\n",
    "It uses the ChatGPT API to generate text in response to prompts specified in the configuration file. The script also uses the stability\\_sdk to generate images based on the prompt specified in the configuration file. The script also uses the Python Imaging Library (PIL) to save the generated images to a directory. Additionally, it processes the generated text, using regular expressions to replace plain backticks with code blocks in the specified programming language.\n",
    "\n",
    "\n",
    "There are several ways to monetize a blog, including the following:\n",
    "\n",
    "1. Advertising: You can place ads on your blog and earn money through pay-per-click or pay-per-impression programs such as Google AdSense.\n",
    "2. Affiliate marketing: You can earn money by promoting other people's products or services and receiving a commission for every sale made through your unique affiliate link.\n",
    "3. Sponsored posts: You can write posts sponsored by a brand or company and receive payment for the promotion.\n",
    "4. Selling products or services: You can sell your own products or services, such as e-books, courses, or consulting services, directly to your readers.\n",
    "5. Crowdfunding: You can use platforms like Patreon or Kickstarter to raise money from your readers to support your blog or other projects.\n",
    "6. Subscription models: You can offer exclusive content, access to a community or other perks for a recurring fee.\n",
    "\n",
    "It's important to note that monetizing your blog requires consistent, high-quality content, and a sizable audience. It may take some time and effort to grow your blog and monetize it successfully.\n",
    "\n",
    "\n",
    "Using AI to generate images can be beneficial in a variety of ways. Some reasons include:\n",
    "\n",
    "1. Automation: AI-generated images can be produced quickly and at scale, reducing the time and resources required to create images manually.\n",
    "2. Variation: AI-generated images can be easily customized to produce a wide range of variations, allowing for greater flexibility in image design.\n",
    "3. Consistency: AI-generated images can be designed to maintain a consistent style or theme, making it easier to maintain a cohesive visual identity across a brand or product.\n",
    "4. Cost-effectiveness: AI-generated images can be less expensive than hiring a professional photographer or graphic designer, making them more accessible to businesses with limited budgets.\n",
    "5. Creative possibilities: AI-generated images can be used in creative ways, such as creating surreal or abstract imagery that would be difficult to achieve through traditional means.\n",
    "6. Personalization: AI-generated images can be used to personalize content for targeted audience, resulting in higher engagement and conversion rates.\n",
    "7. Efficiency: AI-generated images can be used to generate high-quality images for use in various applications, such as product advertising, web design, and mobile apps.\n",
    "\n",
    "Using AI to generate images can be beneficial in a variety of ways. Some reasons include:\n",
    "\n",
    "1. Automation: AI-generated images can be produced quickly and at scale, reducing the time and resources required to create images manually.\n",
    "2. Variation: AI-generated images can be easily customized to produce a wide range of variations, allowing for greater flexibility in image design.\n",
    "3. Consistency: AI-generated images can be designed to maintain a consistent style or theme, making it easier to maintain a cohesive visual identity across a brand or product.\n",
    "4. Cost-effectiveness: AI-generated images can be less expensive than hiring a professional photographer or graphic designer, making them more accessible to businesses with limited budgets.\n",
    "5. Creative possibilities: AI-generated images can be used in creative ways, such as creating surreal or abstract imagery that would be difficult to achieve through traditional means.\n",
    "6. Personalization: AI-generated images can be used to personalize content for targeted audience, resulting in higher engagement and conversion rates.\n",
    "7. Efficiency: AI-generated images can be used to generate high-quality images for use in various applications, such as product advertising, web design, and mobile apps.\n",
    "\n",
    "AI has the potential to change blog writing in the future by making the process more efficient and personalized. For example, AI-powered writing assistants can help writers generate content faster by suggesting topics, outlining structure, and providing research and data to support their ideas. Additionally, AI-powered content generation can personalize blogs by analyzing audience data and tailoring the content to their interests and preferences. It can also generate images, infographics and videos to supplement the written content. However, it's important to note that AI-generated content should be carefully evaluated for accuracy and bias, and should be used in conjunction with human editing and oversight to ensure high-quality and ethical results.\n",
    "\n",
    "### References\n",
    "- https://github.com/FriendlyUser/chatgpt-blog-generator\n",
    "\n",
    "\n",
    "## Investor document downloader\n",
    "\n",
    "CSE exchange refers to the Canadian Securities Exchange, which is a stock exchange located in Canada. The CSE is a fully electronic exchange that provides a marketplace for the trading of securities listed on its platform. It offers a range of listing options, including equity, debt, and structured products, and operates with a focus on serving emerging and growth companies. The exchange aims to provide an efficient, transparent, and accessible marketplace for issuers and investors, and is regulated by the Ontario Securities Commission.\n",
    "\n",
    "\n",
    "Downloading reports from the CSE exchange can provide valuable information for investors, traders, and other market participants. The reports contain data on market activity, financial performance, and other key metrics for the companies listed on the exchange. By accessing these reports, you can gain insight into the financial health and performance of the companies, which can help inform your investment decisions.\n",
    "\n",
    "For example, the CSE provides annual and quarterly financial statements, press releases, and other disclosures for companies listed on the exchange. By downloading and reviewing these reports, you can get a better understanding of a company's financial performance, including revenue, expenses, profits, and growth trends. This information can help you make more informed decisions about whether to buy, sell, or hold a company's stock.\n",
    "\n",
    "Additionally, downloading CSE reports can provide you with insights into the overall health of the Canadian securities market, as well as emerging trends and opportunities. This information can be especially useful for traders and investors looking to make strategic decisions about their portfolios.\n",
    "\n",
    "In summary, downloading reports from the CSE exchange can provide valuable information for market participants and\n",
    "\n",
    "\n",
    "```python \n",
    " import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from cad_tickers.exchanges.cse import get_recent_docs_from_url\n",
    "from extract_doc import mk_dir, handle_logic\n",
    "from io import BytesIO, StringIO\n",
    "\"\"\"\n",
    "1. Get the list of documents from the CSE website\n",
    "2. For each document, get the document url\n",
    "3. If the document url is not in the csv file, add it to the csv file\n",
    "4. For each document url, download the document and add it to the docs folder\n",
    "5. For each document url, make a discord request with the document url\n",
    "6. For each document url, make a discord request with the document summary\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fig_to_buffer(fig):\n",
    "  \"\"\" returns a matplotlib figure as a buffer\n",
    "  \"\"\"\n",
    "  buf = BytesIO()\n",
    "  fig.savefig(buf, format='png')\n",
    "  buf.seek(0)\n",
    "  imgdata = buf.read()\n",
    "  return imgdata\n",
    "\n",
    "def make_discord_request(content, embeds = [], filename = None, file = None):\n",
    "    url = os.getenv(\"DISCORD_WEBHOOK\")\n",
    "    if url == None:\n",
    "        print('DISCORD_WEBHOOK Missing')\n",
    "        pass\n",
    "    data = {}\n",
    "    data[\"content\"] = content\n",
    "    files = {'file': (filename, file, 'application/pdf')}\n",
    "    if filename != None and file != None:\n",
    "        resp = requests.post(\n",
    "            url, data=data, files=files\n",
    "        )\n",
    "    elif len(embeds) != 0:\n",
    "        data[\"embeds\"] = embeds\n",
    "        resp = requests.post(\n",
    "            url, data=json.dumps(data), headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "    print(resp) \n",
    " ```\n",
    "\n",
    "This code is a Python script that downloads documents from the Canadian Securities Exchange (CSE) website and makes a Discord request with the document URL and summary. Here's a brief overview of what it does:\n",
    "\n",
    "1. It retrieves a list of documents from the CSE website using the `get_recent_docs_from_url` function.\n",
    "2. For each document, it retrieves the document URL.\n",
    "3. If the document URL is not already in a CSV file, it adds it to the CSV file.\n",
    "4. It downloads the document and saves it to a directory called `docs`.\n",
    "5. For each document URL, it makes a Discord request with the document URL.\n",
    "6. For each document URL, it makes another Discord request with the document summary.\n",
    "\n",
    "The Discord request is sent using the `requests` library. The `make_discord_request` function takes in several arguments, including the content, embeds, filename, and file data. If the filename and file data are provided, the function makes a request with the file attached. If the embeds argument is provided, the function makes a request with the embeds data. The URL for the Discord webhook is read from an environment variable `DISCORD_WEBHOOK`.\n",
    "\n",
    "\n",
    "```python \n",
    " \n",
    "stockList = [\"PKK\", \"IDK\", \"ADDC\", \"VPH\", \"VST\", \"ACT\"]\n",
    "def get_cse_tickers_data():\n",
    "    url = \"https://github.com/FriendlyUser/cad_tickers_list/blob/main/static/latest/cse.csv?raw=true\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    s = r.content\n",
    "    return pd.read_csv(StringIO(s.decode('utf-8')))\n",
    "\n",
    "stock_df = get_cse_tickers_data()\n",
    "stock_rows = stock_df.loc[stock_df['Symbol'].isin(stockList)]\n",
    "stock_rows.loc[:, 'stock'] = stock_rows['Symbol']\n",
    "stock_rows.loc[:, 'url'] = stock_rows['urls']\n",
    "stockUrls = stock_rows.to_dict('records')\n",
    "csv_file = \"docs.csv\"\n",
    "if os.path.isfile(csv_file):\n",
    "    # read from csv\n",
    "    df = pd.read_csv(csv_file)\n",
    "else:\n",
    "    # make new df\n",
    "    df = pd.DataFrame(columns=[\"stock\", \"url\", \"docUrl\"])\n",
    " \n",
    " ```\n",
    "\n",
    "This code is retrieving data for a list of stocks (`stockList`) from the Canadian Securities Exchange (CSE). It uses the `get_cse_tickers_data` function to retrieve the data from a CSV file hosted on GitHub, which contains information about CSE stocks, such as their symbol, name, and URLs. The function uses the `requests` library to make a GET request to the URL of the CSV file, and the returned content is then converted into a Pandas DataFrame using `pd.read_csv`.\n",
    "\n",
    "Next, the code filters the data for only the stocks in the `stockList` by using the `loc` method on the DataFrame and checking if the value in the `Symbol` column is in the `stockList` using the `isin` method. The filtered data is then saved in the `stock_rows` DataFrame.\n",
    "\n",
    "The code then creates two new columns in the `stock_rows` DataFrame, one named `stock` and the other named `url`, which contain the values from the `Symbol` and `urls` columns, respectively. The filtered data is then converted into a list of dictionaries (`stockUrls`) using the `to_dict` method.\n",
    "\n",
    "Finally, the code checks if there is an existing CSV file named `docs.csv` in the current directory. If there is, it reads the data into a DataFrame using `pd.\n",
    "\n",
    "\n",
    "```python \n",
    " for stock in stockUrls:\n",
    "    stockName = stock.get(\"stock\")\n",
    "    stockUrl = stock.get(\"url\")\n",
    "    urls = []\n",
    "    try:\n",
    "        urls = get_recent_docs_from_url(stockUrl)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    for docUrl in urls:\n",
    "        # skip malformed relative urls\n",
    "        if docUrl[0] == '/':\n",
    "            continue\n",
    "        # add each element to list\n",
    "        exists = docUrl in df[\"docUrl\"].tolist()\n",
    "        if exists == False:\n",
    "            print(f\"Adding {stockName}: {docUrl}\")\n",
    "            df.loc[len(df)] = [stockName, stockUrl, docUrl]\n",
    "            # wrap in todo\n",
    "            stock_doc_dir = f\"docs/{stockName}\"\n",
    "            mk_dir(stock_doc_dir)\n",
    "            stock_doc_file_path = docUrl.split(\"/\")[-1]\n",
    "            pdf_file_name = f\"{stock_doc_dir}/{stock_doc_file_path}.pdf\"\n",
    "            companyName = stock.get(\"Company\").\\\n",
    "                replace('Inc.', '').\\\n",
    "                replace('Pharma', '').\\\n",
    "                strip()\n",
    "            dataDict = {\n",
    "                \"url\": docUrl,\n",
    "                \"path\": pdf_file_name,\n",
    "                \"company_name\": companyName\n",
    "            }\n",
    "            result_obj = handle_logic(dataDict)\n",
    "            file_contents = result_obj.get(\"contents\")\n",
    "            summary = \"N/A\"\n",
    "            try:\n",
    "                summary = result_obj.get(\"summary\")[:1980]\n",
    "            except TypeError as e:\n",
    "                pass\n",
    "            embeds = [\n",
    "                {\n",
    "                    \"title\": stockName,\n",
    "                    \"url\": docUrl,\n",
    "                    \"description\": summary\n",
    "                }\n",
    "            ]\n",
    "            if file_contents != None:\n",
    "                make_discord_request(f\"*{stockName}*: \\n {docUrl}\", embeds, pdf_file_name, file_contents)\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                make_discord_request(f\"*{stockName}*: \\n {docUrl}\", embeds)\n",
    "                time.sleep(1)\n",
    "            pass\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df = df.sort_values(by=['stock'])\n",
    "df.to_csv(csv_file, index=False) \n",
    " ```\n",
    "\n",
    "This code is a script that downloads and summarizes recent documents from a list of stock symbols. The stock symbols are stored in the list `stockList`, and the data for these symbols is obtained by calling the `get_cse_tickers_data` function, which retrieves data from a CSV file stored on GitHub.\n",
    "\n",
    "The code then loops through each of the stocks in `stockList`, and for each stock, it retrieves a list of recent documents by calling the `get_recent_docs_from_url` function and passing it the URL for the stock.\n",
    "\n",
    "For each document, the code checks if the document URL already exists in the DataFrame `df`. If the URL does not exist, the code adds the document information (i.e., stock name, URL, and a summary) to `df` and makes a Discord request to post the information to a Discord channel. The information posted to the Discord channel includes the stock name, document URL, and a summary of the document's contents. If the contents of the document are available, they are also included in the Discord request as an attachment.\n",
    "\n",
    "After all the documents have been processed, the code removes any duplicates in `df` and sorts the entries by the stock name. Finally, the updated `df` is saved to a CSV file.\n",
    "\n",
    "###  References\n",
    "- https://github.com/dli-invest/cse_file_downloader/blob/main/scrap_cse_releases.py\n",
    "\n",
    "\n",
    "## Generate Subtitles for Youtube Videos\n",
    "Transcribing videos is important for several reasons. Firstly, it makes the content of the video more accessible to people who are deaf or hard of hearing. Secondly, it can improve search engine optimization (SEO) as the text in the transcription can be indexed by search engines. Thirdly, it can make it easier to create captions or subtitles for the video, which can again make it more accessible to a wider audience. Finally, transcribing videos can be useful for researchers or content creators who want to analyze the content of the video or repurpose it in other ways.\n",
    "\n",
    "\n",
    "```python \n",
    " import whisper\n",
    "import gradio as gr\n",
    "import ffmpeg\n",
    "from yt_dlp import YoutubeDL\n",
    "import os\n",
    "import sys\n",
    "from subprocess import PIPE, run\n",
    "\n",
    "youtube_livestream_codes = [\n",
    "    91,\n",
    "    92,\n",
    "    93,\n",
    "    94,\n",
    "    95,\n",
    "    96,\n",
    "    300,\n",
    "    301,\n",
    "]\n",
    "youtube_mp4_codes = [\n",
    "    298,\n",
    "    18,\n",
    "    22,\n",
    "    140,\n",
    "    133,\n",
    "    134\n",
    "]\n",
    "\n",
    "def second_to_timecode(x: float) -> str:\n",
    "    hour, x = divmod(x, 3600)\n",
    "    minute, x = divmod(x, 60)\n",
    "    second, x = divmod(x, 1)\n",
    "    millisecond = int(x * 1000.)\n",
    "\n",
    "    return '%.2d:%.2d:%.2d,%.3d' % (hour, minute, second, millisecond)\n",
    "\n",
    "def get_video_metadata(video_url: str = \"https://www.youtube.com/watch?v=21X5lGlDOfg&ab_channel=NASA\")-> dict:\n",
    "    with YoutubeDL({'outtmpl': '%(id)s.%(ext)s'}) as ydl:\n",
    "        info_dict = ydl.extract_info(video_url, download=False)\n",
    "        video_title = info_dict.get('title', None)\n",
    "        uploader_id = info_dict.get('uploader_id', None)\n",
    "        print(f\"[youtube] {video_title}: {uploader_id}\")\n",
    "    return info_dict\n",
    "\n",
    "\n",
    "def parse_metadata(metadata) -> dict:\n",
    "    \"\"\"\n",
    "    Parse metadata and send to discord.\n",
    "    After a video is done recording, \n",
    "    it will have both the livestream format and the mp4 format.\n",
    "    \"\"\"\n",
    "    # send metadata to discord\n",
    "    formats = metadata.get(\"formats\", [])\n",
    "    # filter for ext = mp4\n",
    "    mp4_formats = [f for f in formats if f.get(\"ext\", \"\") == \"mp4\"]\n",
    "    try:\n",
    "        format_ids = [int(f.get(\"format_id\", 0)) for f in mp4_formats]\n",
    "        video_entries = sorted(set(format_ids).intersection(youtube_mp4_codes))\n",
    "\n",
    "        is_livestream = True\n",
    "        if len(video_entries) > 0:\n",
    "            # use video format id over livestream id if available\n",
    "            selected_id = video_entries[0]\n",
    "            is_livestream = False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        selected_id = mp4_formats[0].get(\"format_id\")\n",
    "        is_livestream = False\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"selected_id\": selected_id,\n",
    "        \"is_livestream\": is_livestream,\n",
    "    } \n",
    " ```\n",
    "\n",
    "The above code is written in Python and contains functions for getting metadata for a YouTube video, parsing the metadata, and converting seconds to timecode. It also imports the necessary libraries including `whisper`, `gradio`, `ffmpeg`, `yt_dlp`, `os`, `sys`, and `subprocess`.\n",
    "\n",
    "The `get_video_metadata` function takes a YouTube video URL as input and returns a dictionary containing metadata about the video such as the title and uploader ID. This function uses the `YoutubeDL` library to extract information about the video from YouTube.\n",
    "\n",
    "The `parse_metadata` function takes the metadata dictionary as input and returns a dictionary containing the selected video format ID and a boolean indicating whether the video is a livestream or not. This function filters the available video formats to only include MP4 formats and then selects the video format with the highest priority (based on the priority list `youtube_mp4_codes`). If no MP4 format is available, it selects the first available format.\n",
    "\n",
    "The `second_to_timecode` function takes a floating-point number representing a time in seconds as input and returns a string formatted as `hh:mm:ss,ms` (hours:minutes:seconds,milliseconds) where `ms` is the milliseconds portion of the time.\n",
    "\n",
    "Note that the code also includes a list of YouTube format codes for livestreams (`youtube_livestream_codes`) and MP4 formats (`youtube_mp4_codes`).\n",
    "\n",
    "\n",
    "```python \n",
    " def get_video(url: str, config: dict):\n",
    "    \"\"\"\n",
    "    Get video from start time.\n",
    "    \"\"\"\n",
    "    # result = subprocess.run()\n",
    "    # could delay start time by a few seconds to just sync up and capture the full video length\n",
    "    # but would need to time how long it takes to fetch the video using youtube-dl and other adjustments and start a bit before\n",
    "    filename = config.get(\"filename\", \"livestream01.mp4\")\n",
    "    end = config.get(\"end\", \"00:15:00\")\n",
    "    overlay_file = ffmpeg.input(filename)\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(url, t=end)\n",
    "        .output(filename)\n",
    "        .run()\n",
    "    )\n",
    "\n",
    "def get_all_files(url: str, end: str = \"00:15:00\"):\n",
    "    metadata = get_video_metadata(url)\n",
    "    temp_dict = parse_metadata(metadata)\n",
    "    selected_id = temp_dict.get(\"selected_id\", 0)\n",
    "    formats = metadata.get(\"formats\", [])\n",
    "    selected_format = [f for f in formats if f.get(\"format_id\", \"\") == str(selected_id)][0]\n",
    "    format_url = selected_format.get(\"url\", \"\")\n",
    "    filename = \"temp.mp4\"\n",
    "    get_video(format_url, {\"filename\": filename, \"end\": end})\n",
    "    return filename\n",
    "\n",
    "def get_text_from_mp3_whisper(inputType:str, mp3_file: str, url_path: str, taskName: str, srcLanguage: str)->str:\n",
    "    # remove the file if it exists\n",
    "    if os.path.exists(\"transcript.srt\"):\n",
    "        os.remove(\"transcript.srt\")\n",
    "    \n",
    "    if os.path.exists(\"temp.mp4\"):\n",
    "        os.remove(\"temp.mp4\")\n",
    "    \n",
    "    if os.path.exists(\"subtitled.mp4\"):\n",
    "        os.remove(\"subtitled.mp4\")\n",
    "    \n",
    "    model = whisper.load_model(\"medium\")\n",
    "    # options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "    options = dict(language=srcLanguage)\n",
    "    transcribe_options = dict(task=taskName, **options)\n",
    "    # return if url_path is not set, taskName is not set, srcLanguage is not set\n",
    "    if inputType == \"url\":\n",
    "        filename = get_all_files(url_path)\n",
    "        print(\"Retrieved the file\")\n",
    "        result = model.transcribe(filename, **transcribe_options)\n",
    "        print(\"transcribing the file\")\n",
    "    else:\n",
    "        result = model.transcribe(mp3_file, **transcribe_options)\n",
    "    # adjust for spacy mode\n",
    "    html_text = \"\"\n",
    "    lines = []\n",
    "    for count, segment in enumerate(result.get(\"segments\")):\n",
    "        # print(segment)\n",
    "        start = segment.get(\"start\")\n",
    "        end = segment.get(\"end\")\n",
    "        lines.append(f\"{count}\")\n",
    "        lines.append(f\"{second_to_timecode(start)} --> {second_to_timecode(end)}\")\n",
    "        lines.append(segment.get(\"text\", \"\").strip())\n",
    "        lines.append('')\n",
    "    words = '\\n'.join(lines)\n",
    "    # save to transcript.srt\n",
    "    with open(\"transcript.srt\", \"w\") as f:\n",
    "        f.write(words)\n",
    "    print(\"done transcribing\")\n",
    "\n",
    "    input_file = 'temp.mp4'\n",
    "    subtitles_file = 'transcript.srt'\n",
    "    output_file = 'subtitled.mp4'\n",
    "    try:\n",
    "        print(\"attempt to output file\")\n",
    "        (\n",
    "            ffmpeg\n",
    "            .input(input_file)\n",
    "            .filter('subtitles', subtitles_file)\n",
    "            .output(output_file)\n",
    "            .run()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"failed to output file\")\n",
    "        print(e)\n",
    "        output_file = \"temp.mp4\"\n",
    "    # return temp.mp4\n",
    "    \n",
    "    return result.get(\"segments\"), words, output_file \n",
    " ```\n",
    "\n",
    "The `get_text_from_mp3_whisper` function takes several arguments including `inputType`, `mp3_file`, `url_path`, `taskName`, and `srcLanguage`. It first removes any existing files with names `transcript.srt`, `temp.mp4`, and `subtitled.mp4`.\n",
    "\n",
    "It then loads a pre-trained model from `whisper` using the `load_model` function. The function checks the value of `inputType` to determine whether to transcribe a local mp3 file (`mp3_file`) or a video at a remote URL (`url_path`). If `inputType` is `\"url\"`, it downloads the video from the remote URL using the `get_all_files` function and transcribes it. If `inputType` is `\"file\"`, it transcribes the local mp3 file.\n",
    "\n",
    "The transcribed segments are then saved to a file named `transcript.srt`. The function then attempts to add subtitles to the video by overlaying the saved SRT file onto a copy of the video file. The resulting file is saved as `subtitled.mp4`. If the overlaying process fails, it saves the resulting file as `temp.mp4`.\n",
    "\n",
    "The function then returns the segments and the contents of the SRT file as a string, as well as the name of the resulting video file.\n",
    "\n",
    "\n",
    "```python \n",
    " gr.Interface(\n",
    "    title = 'Download Video From url and extract text from audio', \n",
    "    fn=get_text_from_mp3_whisper, \n",
    "    inputs=[\n",
    "        gr.Dropdown([\"url\", \"file\"], value=\"url\"),\n",
    "        gr.inputs.Audio(type=\"filepath\"),\n",
    "        gr.inputs.Textbox(),\n",
    "        gr.Dropdown([\"translate\", \"transcribe\"], value=\"translate\"),\n",
    "        gr.Dropdown([\"Japanese\", \"English\"], value=\"Japanese\")\n",
    "    ],\n",
    "    button_text=\"Go!\",\n",
    "    button_color=\"#333333\",\n",
    "    outputs=[\n",
    "        \"json\", \"text\", \"file\"\n",
    "    ], \n",
    " ```\n",
    "\n",
    "It seems that the code block is not complete, as there is no closing parenthesis for the `gr.Interface` function call. However, assuming that the rest of the code is present, this function call creates a user interface using the `gradio` library.\n",
    "\n",
    "The interface has a title, \"Download Video From url and extract text from audio\", and it takes in the following inputs:\n",
    "\n",
    "1. A dropdown menu to select the input type (\"url\" or \"file\").\n",
    "2. An audio input to upload an audio file.\n",
    "3. A textbox to enter the URL of a video.\n",
    "4. A dropdown menu to select the task (\"translate\" or \"transcribe\").\n",
    "5. A dropdown menu to select the source language (\"Japanese\" or \"English\").\n",
    "\n",
    "The interface also has a button with text \"Go!\" and color \"#333333\", and it outputs a JSON object, a string of text, and a file.\n",
    "\n",
    "\n",
    "Using AI to transcribe videos has many practical benefits. Firstly, it enables the creation of accurate, searchable and accessible transcripts that can be used for a variety of purposes. Transcripts can be used to help people with hearing impairments to understand video content, as well as to provide captions and subtitles for non-native speakers of the video's language. Transcripts can also be used to provide metadata for video content, making it easier to search and categorize. Additionally, transcripts can be used to analyze the content of video content, allowing for more effective search and retrieval of relevant information.\n",
    "\n",
    "AI-based transcription is also faster and more cost-effective than traditional human transcription methods. With the advances in AI technology, transcription can be done much faster and at a lower cost than hiring human transcribers. This allows for more content to be transcribed in a shorter amount of time, and at a lower cost. Furthermore, AI-based transcription can be easily scaled up or down depending on the size of the content that needs to be transcribed.\n",
    "\n",
    "Overall, using AI to transcribe videos is a useful and practical application of AI technology that has the potential to significantly improve the accessibility and usability of video content.\n",
    "\n",
    "### References\n",
    "- https://huggingface.co/spaces/FriendlyUser/YoutubeDownloaderSubber\n",
    "\n",
    "## Scrapping Superbowl results with pandas\n",
    "\n",
    "The Super Bowl is the championship game of the National Football League (NFL), which is played annually to determine the winner of the NFL season. The game is played on the first Sunday in February and is considered the biggest sporting event in the United States, attracting millions of viewers each year. The Super Bowl is also one of the largest events in the world of advertising, with many of the most expensive and highly-anticipated commercials airing during the broadcast. In addition to the game itself, the Super Bowl weekend has become a major cultural event, with parties, concerts, and other festivities taking place in the host city leading up to the game.\n",
    "\n",
    "\n",
    "```python \n",
    " import requests\n",
    "import pandas\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "# read pandas dataframe from url\n",
    "\n",
    "base_url = 'http://www.espn.com/nfl/history'\n",
    "leaders_df = pandas.read_html(f\"{base_url}/leaders\", attrs={'class': 'tablehead'})[0]\n",
    "\n",
    "\n",
    "winners_df = pandas.read_html(f\"http://www.espn.com/nfl/superbowl/history/winners\", attrs={'class': 'tablehead'})[0]\n",
    "\n",
    "\n",
    "mvp_df = pandas.read_html(\"http://www.espn.com/nfl/superbowl/history/mvps\", attrs={'class': 'tablehead'})[0]\n",
    "\n",
    "\n",
    "print(leaders_df.head())\n",
    "print(winners_df.head())\n",
    "print(mvp_df.head()) \n",
    " ```\n",
    "\n",
    "This code uses the `requests`, `pandas`, and `matplotlib` libraries to retrieve and process data from three different URLs related to NFL history and the Super Bowl.\n",
    "\n",
    "The first URL is for NFL leaders and the data is loaded into a pandas DataFrame called `leaders_df`. The second URL is for Super Bowl winners and the data is loaded into a pandas DataFrame called `winners_df`. The third URL is for Super Bowl MVPs and the data is loaded into a pandas DataFrame called `mvp_df`.\n",
    "\n",
    "Finally, the code prints the first five rows of each DataFrame using the `head()` method of pandas DataFrames, which returns the first n (in this case, n=5) rows of the DataFrame.\n",
    "\n",
    "\n",
    "```python \n",
    "                    0                    1                  2\n",
    "0  Touchdown Leaders    Touchdown Leaders  Touchdown Leaders\n",
    "1                 RK               PLAYER                 TD\n",
    "2                  1           Jerry Rice                208\n",
    "3                  2         Emmitt Smith                175\n",
    "4                  3  LaDainian Tomlinson                162\n",
    "                                0  ...                               3\n",
    "0  Super Bowl Winners and Results  ...  Super Bowl Winners and Results\n",
    "1                             NO.  ...                          RESULT\n",
    "2                               I  ...    Green Bay 35, Kansas City 10\n",
    "3                              II  ...        Green Bay 33, Oakland 14\n",
    "4                             III  ...   New York Jets 16, Baltimore 7\n",
    "\n",
    "[5 rows x 4 columns]\n",
    "                                  0  ...                                 2\n",
    "0  Super Bowl Most Valuable Players  ...  Super Bowl Most Valuable Players\n",
    "1                               NO.  ...                        HIGHLIGHTS\n",
    "2                                 I  ...              Two touchdown passes\n",
    "3                                II  ...           202 yards passing, 1 TD\n",
    "4                               III  ...                 206 yards passing \n",
    " ```\n",
    "\n",
    "These are the first five rows of each of the three pandas DataFrames that were created from the URLs related to NFL history and the Super Bowl.\n",
    "\n",
    "The first DataFrame, `leaders_df`, shows the NFL touchdown leaders, including the rank, player name, and number of touchdowns scored.\n",
    "\n",
    "The second DataFrame, `winners_df`, shows the results of each Super Bowl game, including the number of the Super Bowl, the winning team, and the result.\n",
    "\n",
    "The third DataFrame, `mvp_df`, shows the most valuable player of each Super Bowl, including the number of the Super Bowl, the player's name, and highlights of their performance.\n",
    "\n",
    "\n",
    "```python \n",
    " # grab 4th column\n",
    "# pandas drop first row of winners_df and change column names\n",
    "# save winners_df to csv\n",
    "winners_df.to_csv('winners_raw.csv', index=False)\n",
    "winners_df = winners_df.iloc[1:]\n",
    "winners_df.columns = winners_df.iloc[0]\n",
    "winners_df = winners_df.iloc[1:]\n",
    "print(winners_df.columns)\n",
    "winners_df.to_csv('winners.csv', index=False)\n",
    "print(winners_df.head())\n",
    "\n",
    "sites_of_superbowl = winners_df['SITE'].unique()\n",
    "\n",
    "print(sites_of_superbowl)\n",
    "# for date column find the number of instances of Orange Bowl and plot it\n",
    "# adjust all site to drop content in () and save to csv\n",
    "\n",
    "winners_df_adjusted = winners_df.copy()\n",
    "winners_df_adjusted['SITE'] = winners_df_adjusted['SITE'].str.replace(r'\\(.*\\)', '')\n",
    "fig =  winners_df_adjusted.groupby('SITE').size().plot(kind='bar', figsize=(10, 18)).get_figure() \n",
    "```\n",
    "\n",
    "This code continues processing the `winners_df` DataFrame, which contains information about the results of each Super Bowl game.\n",
    "\n",
    "The 4th column of `winners_df` is not grabbed because the code does not contain any specific instructions to do so.\n",
    "\n",
    "The first row of `winners_df` is dropped using the `iloc` method and the remaining rows are set as the new column names using the `columns` attribute. This DataFrame is then saved to a new CSV file called 'winners.csv' using the `to_csv` method of pandas DataFrames.\n",
    "\n",
    "Next, the code uses the `unique` method of pandas Series to get the unique values of the 'SITE' column and store the result in a variable called `sites_of_superbowl`.\n",
    "\n",
    "Then, a new DataFrame called `winners_df_adjusted` is created as a copy of `winners_df` and the 'SITE' column is adjusted to remove the content in parentheses using the `str.replace` method of pandas Series.\n",
    "\n",
    "Finally, the code uses the `groupby` method of pandas DataFrames to group the DataFrame by the 'SITE' column and the `size` method to get the size of each group. The resulting pandas Series is plotted using the `plot` method with the `kind` argument set to 'bar' and the `figsize` argument set to (10, 18). The figure object is stored in a variable called `fig`.\n",
    "\n",
    "\n",
    "```python \n",
    " Index(['NO.', 'DATE', 'SITE', 'RESULT'], dtype='object', name=1)\n",
    "1  NO.  ...                         RESULT\n",
    "2    I  ...   Green Bay 35, Kansas City 10\n",
    "3   II  ...       Green Bay 33, Oakland 14\n",
    "4  III  ...  New York Jets 16, Baltimore 7\n",
    "5   IV  ...    Kansas City 23, Minnesota 7\n",
    "6    V  ...        Baltimore 16, Dallas 13\n",
    "\n",
    "[5 rows x 4 columns]\n",
    "['Los Angeles Memorial Coliseum' 'Orange Bowl (Miami)'\n",
    " 'Tulane Stadium (New Orleans)' 'Rice Stadium (Houston)'\n",
    " 'Rose Bowl (Pasadena, Calif.)' 'Superdome (New Orleans)'\n",
    " 'Silverdome (Pontiac, Mich.)' 'Tampa (Fla.) Stadium'\n",
    " 'Stanford (Calif.) Stadium' 'Jack Murphy Stadium (San Diego)'\n",
    " 'Joe Robbie Stadium (Miami)' 'Metrodome (Minneapolis)'\n",
    " 'Georgia Dome (Atlanta)' 'Sun Devil Stadium (Tempe, Ariz.)'\n",
    " 'Qualcomm Stadium (San Diego)' 'Pro Player Stadium (Miami)'\n",
    " 'Raymond James Stadium (Tampa, Fla.)' 'Reliant Stadium (Houston)'\n",
    " 'Alltel Stadium (Jacksonville, Fla.)' 'Ford Field (Detroit)'\n",
    " 'Dolphin Stadium (Miami)'\n",
    " 'University of Phoenix Stadium (Glendale, Ariz.)'\n",
    " 'Sun Life Stadium (Miami)' 'Cowboys Stadium (Arlington, Texas)'\n",
    " 'Lucas Oil Stadium (Indianapolis)'\n",
    " 'Mercedes-Benz Superdome (New Orleans)'\n",
    " 'MetLife Stadium (East Rutherford, N.J.)'\n",
    " \"Levi's Stadium (Santa Clara, Calif.)\" 'NRG Stadium (Houston)'\n",
    " 'U.S. Bank Stadium (Minneapolis)' 'Mercedes-Benz Stadium (Atlanta)'\n",
    " 'Hard Rock Stadium (Miami)' 'SoFi Stadium (Inglewood, Calif.)' \n",
    " ```\n",
    "\n",
    "The code above uses the `pandas` library to scrape data about the NFL Super Bowl winners, MVPs, and touchdown leaders from the ESPN website. It then saves the data in CSV format for future use.\n",
    "\n",
    "It starts by reading the HTML tables from the website using the `read_html` method from the `pandas` library and storing the results in dataframes. The first dataframe, `leaders_df`, contains the touchdown leaders. The second dataframe, `winners_df`, contains the Super Bowl winners and results. The third dataframe, `mvp_df`, contains the Super Bowl MVPs.\n",
    "\n",
    "The code then processes the `winners_df` dataframe to remove the first row and set the column names, which are the first row of the dataframe. The processed dataframe is then saved in a new CSV file called 'winners.csv'.\n",
    "\n",
    "Next, the code extracts the unique values of the 'SITE' column of the `winners_df` dataframe and stores the results in the `sites_of_superbowl` array. The code then creates a new dataframe called `winners_df_adjusted` that is a copy of the `winners_df` dataframe but with the content in the parentheses removed from the 'SITE' column. Finally, the code plots a bar chart of the number of instances of each site using the `groupby` and `size` methods from the `pandas` library and the `plot` method from the `matplotlib` library.\n",
    "\n",
    "\n",
    "![football graph](https://raw.githubusercontent.com/FriendlyUser/data-science-projects/main/superbowl/winners.png)\n",
    "Yes, indeed! Web scraping is a powerful tool to collect data from websites and the combination with data visualization makes it even more valuable to understand and draw insights from the data. It's also a great way to automate data collection and analysis tasks.\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "- https://github.com/FriendlyUser/data-science-projects/tree/main/superbowl\n",
    "\n",
    "\n",
    "## Getting youtube livestream status\n",
    "\n",
    "YouTube is a video-sharing website where users can upload, share, and view videos. It was founded in February 2005 and later acquired by Google in November 2006. It has since become one of the largest and most popular websites on the internet, offering a wide variety of content, including music videos, educational videos, movie trailers, and more. Users can also interact with each other by commenting on videos, giving \"thumbs up\" or \"thumbs down\" ratings, and subscribing to other users' channels. YouTube is available on various devices, including computers, smartphones, and smart TVs.\n",
    "\n",
    "Transcribing Federal Reserve livestreams in real time is important for investors because the statements made by Federal Reserve officials can have a significant impact on financial markets. The Federal Reserve is the central bank of the United States, and its policies and statements on monetary policy, economic conditions, and interest rates can have a major effect on the stock market, the bond market, and the value of the US dollar.\n",
    "\n",
    "By transcribing the livestreams in real time, investors can quickly and accurately obtain information and insights from the Federal Reserve's statements. This can help them make informed investment decisions and respond to market changes as they occur. Additionally, real-time transcription allows investors to more effectively analyze and interpret the Federal Reserve's statements, giving them a competitive advantage in a fast-paced and constantly-changing market.\n",
    "\n",
    "In short, transcribing Federal Reserve livestreams in real time is important for investors because it allows them to stay up-to-date on the central bank's statements and respond quickly to market changes, which can have a significant impact on their investments.\n",
    "\n",
    "\n",
    "```python \n",
    " \"\"\"\n",
    "author: FriendlyUser\n",
    "description: grab livestream data from url using selenium, (need a browser for youtube)\n",
    "create database entries to track livestreams, check if livestream is live or upcoming and exclude certain channels with never ending livestreams.\n",
    "\"\"\"\n",
    "\n",
    "import bs4\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import dateparser\n",
    "\n",
    "def get_livestreams_from_html(data: str):\n",
    "    \"\"\"\n",
    "        gets livestream from html from youtube channel and determines if it is live or upcoming.\n",
    "        Returns dict:\n",
    "          time: time of livestream\n",
    "          channel: channel name\n",
    "          status: LIVE or UPCOMING or none\n",
    "    \"\"\"\n",
    "    # get text data from url using requests\n",
    "    try:\n",
    "\n",
    "        soup = bs4.BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "        livestream_data = []\n",
    "        first_section = soup.find(\"ytd-item-section-renderer\")\n",
    "\n",
    "        title_wrapper = first_section.find(\"ytd-channel-video-player-renderer\")\n",
    "        if title_wrapper == None:\n",
    "            watch_link = first_section.find(\"a\", {\"class\": \"yt-simple-endpoint style-scope ytd-video-renderer\"})\n",
    "            watch_url = watch_link.get(\"href\")\n",
    "        else:\n",
    "            channel_title = title_wrapper.find(\"yt-formatted-string\")\n",
    "            watch_link = channel_title.find(\"a\")\n",
    "            watch_url = watch_link.get(\"href\")\n",
    "        # get video_id \n",
    "        ytd_thumbnail_overlay_time_status_renderer = first_section.find(\"ytd-thumbnail-overlay-time-status-renderer\")\n",
    "        # try to find ytd-video-renderer and get href\n",
    "\n",
    "        if ytd_thumbnail_overlay_time_status_renderer is None:\n",
    "            # try to grab upcoming livestream\n",
    "            scheduled_text = first_section.find(\"ytd-video-meta-block\")\n",
    "            run_time = scheduled_text.get_text()\n",
    "            # parse strings like August 22 at 6:00 AM\n",
    "            # remove words like at\n",
    "            run_str = run_time.replace(\"Scheduled for\", \"\").strip()\n",
    "            parsed_date =  dateparser.parse(run_str)\n",
    "            livestream_data.append({\"date\": parsed_date, \"status\": \"UPCOMING\", \"watch_url\": watch_url})\n",
    "        else:\n",
    "            livestream_label = ytd_thumbnail_overlay_time_status_renderer.get_text()\n",
    "            if livestream_label is not None:\n",
    "                livestream_data.append({\"date\": None, \"status\": livestream_label.strip(), \"watch_url\": watch_url})\n",
    "        return livestream_data\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error getting data from url\")\n",
    "        return [] \n",
    " ```\n",
    "\n",
    "This code uses the BeautifulSoup library and Selenium WebDriver to scrape a YouTube channel for live stream data. The function `get_livestreams_from_html` takes in an HTML string as input and returns a list of dictionaries, each representing a live stream event.\n",
    "\n",
    "The function first creates a BeautifulSoup object from the input HTML string and then searches the HTML tree for specific elements that contain information about the live streams. It checks if the live stream is currently happening or if it's scheduled to happen in the future, and if it's live, it checks if it's never ending. If the live stream is never ending, the function does not include that event in the list of live streams.\n",
    "\n",
    "The function uses the dateparser library to parse dates from strings, such as \"August 22 at 6:00 AM\". The live stream events are returned as a list of dictionaries, each containing the date and time of the event, the channel name, and the status of the event (live or upcoming).\n",
    "\n",
    "This code can be used as a starting point for a more comprehensive script that scrapes live streams from YouTube, but it may need to be adapted to fit the specific needs of the user.\n",
    "\n",
    "\n",
    "```python \n",
    " \n",
    "def get_webdriver():\n",
    "    remote_url = os.environ.get(\"REMOTE_SELENIUM_URL\")\n",
    "    if remote_url is None:\n",
    "        raise Exception(\"Missing REMOTE_SELENIUM_URL in env vars\")\n",
    "    return webdriver.Remote(\n",
    "        command_executor=remote_url,\n",
    "    )\n",
    "\n",
    "def get_html_from_url(url: str):\n",
    "    \"\"\"\n",
    "        gets html from url\n",
    "    \"\"\"\n",
    "    # get text data from url using requests\n",
    "    driver = get_webdriver()\n",
    "    driver.get(url)\n",
    "    time.sleep(10)\n",
    "    # return html from page source\n",
    "    return driver.page_source\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    html = get_html_from_url(\"https://www.youtube.com/c/YahooFinance\")\n",
    "    livestream_data = get_livestreams_from_html(html)\n",
    "\n",
    "    base_url = \"https://www.youtube.com\"\n",
    "    # check if LIVE or UPCOMING\n",
    "    for livestream in livestream_data:\n",
    "        if livestream[\"status\"] == \"LIVE\":\n",
    "            print(\"LIVE\")\n",
    "            youtube_url = base_url + livestream[\"watch_url\"]\n",
    "            data = {\"youtube_url\": youtube_url, \"iteration\": -1, \"table_name\": \"YahooFinance\"}\n",
    "            print(data)\n",
    "        elif livestream[\"status\"] == \"UPCOMING\":\n",
    "            print(\"UPCOMING\")\n",
    "            exit(0)\n",
    "        else:\n",
    "            print(\"NONE\")\n",
    "            exit(1) \n",
    " ```\n",
    "\n",
    "This code is using the Selenium WebDriver to automate the web browser and retrieve the HTML content of a YouTube channel. The channel URL is hardcoded as \"<https://www.youtube.com/c/YahooFinance>\" in the `get_html_from_url` function, but it could be updated to read from a configuration file, as noted in the TODO comment.\n",
    "\n",
    "After retrieving the HTML, the code uses the `get_livestreams_from_html` function to extract information about the livestreams on the channel, if any. It then checks the status of each livestream, and if the status is \"LIVE\", it prints the information, including the URL of the livestream on YouTube and some additional data. If the status is \"UPCOMING\", the code exits with status code 0. If the status is neither \"LIVE\" nor \"UPCOMING\", the code exits with status code 1.\n",
    "\n",
    "### References\n",
    "- https://github.com/dli-invest/fdrtt/blob/main/livestream_scrapper.py\n",
    "- https://github.com/dli-invest/fdrtt"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "source_map": [
   13
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}